{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yQQTA9IGDt8"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX9n-Zed8G_T"
   },
   "source": [
    "# Lab 07: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "- How to convert PyTorch models into portable TorchScript binaries\n",
    "- How to use `gradio` to make a simple demo UI for your ML-powered applications\n",
    "- How to split out a model service from the frontend and spin up a publicly accessible application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45D6GuSwvT7d"
   },
   "outputs": [],
   "source": [
    "lab_idx = 7\n",
    "\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzi8qYKI-njP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow along with a video walkthrough on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "IFrame(src=\"https://fsdl.me/2022-lab-07-video-embed\", width=\"100%\", height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAw7BEI_sCZZ"
   },
   "source": [
    "# Making the model portable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zL0K2Xe-MWJ"
   },
   "source": [
    "While training the model,\n",
    "we've saved checkpoints and stored them locally\n",
    "and on W&B.\n",
    "\n",
    "From these checkpoints, we can reload model weights\n",
    "and even restart training if we are in or can recreate\n",
    "the model development environment.\n",
    "\n",
    "We could directly deploy these checkpoints into production,\n",
    "but they're suboptimal for two reasons.\n",
    "\n",
    "First, as the name suggests,\n",
    "these \"checkpoints\" are designed for serializing\n",
    "state at a point of time in training.\n",
    "\n",
    "That means they can include lots of information\n",
    "not relevant during inference,\n",
    "e.g. optimizer states like running average gradients.\n",
    "\n",
    "Additionally, the model development environment\n",
    "is much more heavyweight than what we need during inference.\n",
    "\n",
    "For example, we've got Lightning for training models\n",
    "and W&B for tracking training runs.\n",
    "\n",
    "These in turn incur dependencies on lots of heavy data science libraries.\n",
    "\n",
    "We don't need this anymore -- we just want to run the model.\n",
    "\n",
    "These are effectively \"compiler tools\", which our runtime model doesn't need.\n",
    "\n",
    "So we need a new model binary artifact for runtime\n",
    "that's leaner and more independent.\n",
    "\n",
    "For this purpose, we use TorchScript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bMPqKDjs623"
   },
   "source": [
    "## Compiling models to TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d9EmZ0j_AQF"
   },
   "source": [
    "Torch has two main facilities for creating\n",
    "more portable model binaries:\n",
    "_scripting_ and _tracing_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9PVzwjQ_YHg"
   },
   "source": [
    "Scripting produces a binary that combines\n",
    "constant `Tensor` values\n",
    "(like weights and positional embeddings)\n",
    "with a program that describes how to use them.\n",
    "\n",
    "The result is a program that creates a dynamic graph,\n",
    "as does a normal PyTorch program,\n",
    "but this program is written in a\n",
    "sub-dialect of Python called\n",
    "_TorchScript_.\n",
    "\n",
    "The [TorchScript sub-dialect of Python](https://pytorch.org/docs/stable/jit_language_reference.html#language-reference)\n",
    "is more performant\n",
    "and can even be run without a Python interpreter.\n",
    "\n",
    "For example, TorchScript programs can be executed in pure C++\n",
    "[using LibTorch](https://pytorch.org/tutorials/advanced/cpp_export.html).\n",
    "\n",
    "You can read more in the documentation for the primary method\n",
    "for scripting models, `torch.jit.script`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1VtGt_Xj_H7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.jit.script??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUOm7G9ESi4s"
   },
   "source": [
    "The primary alternative to scripting is _tracing_,\n",
    "which runs the PyTorch module on a specific\n",
    "set of inputs and records, or \"traces\",\n",
    "the compute graph.\n",
    "\n",
    "You can read more about it in the documentation for the primary method\n",
    "for tracing models, `torch.jit.trace`,\n",
    "or just read the quick summary and comparison below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn3QLOFNjuOa"
   },
   "outputs": [],
   "source": [
    "torch.jit.trace??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing versus Scripting for TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP4TfihfBw9z"
   },
   "source": [
    "The traced program is generally faster than the scripted version,\n",
    "for models that are compatible with both tracing and scripting.\n",
    "\n",
    "Tracing produces a static compute graph,\n",
    "which means all control flow\n",
    "(`if`s or `for` loops)\n",
    "are effectively inlined.\n",
    "\n",
    "As written, our text recognizer has a loop with conditional breaking -- fairly typical for Transformers in autoregressive mode --\n",
    "so it isn't compatible with scripting.\n",
    "\n",
    "Furthermore, the static compute graph includes concrete choices of operations,\n",
    "e.g. specific CUDA kernels if tracing is run on the GPU.\n",
    "\n",
    "If you try to run the traced model on a system that doesn't support those kernels,\n",
    "it will crash.\n",
    "That means tracing must occur in the target deployment environment.\n",
    "\n",
    "Scripted models are much more portable, at the cost of both slower runtimes\n",
    "for a fixed hardware target and of some restrictions on how dynamic the Python code can be.\n",
    "\n",
    "We don't find the restrictions scripting places on Python code to be too onerous\n",
    "and in our experience, the performance gains are not worth the extra effort\n",
    "until the team size is larger,\n",
    "model serving hardware and strategy is more mature,\n",
    "and model release cycles are slower.\n",
    "\n",
    "For an alternative perspective that's more in favor of tracing\n",
    "and walks through how to mix-and-match scripting\n",
    "and tracing for maximum flexibility and performance, see\n",
    "[this blogpost](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/)\n",
    "from\n",
    "[Detectron2](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)\n",
    "dev Yuxin Wu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDARv-GdqtET"
   },
   "source": [
    "Choosing just one of scripting or tracing\n",
    "means we can use a high-level method\n",
    "from PyTorch Lightning,\n",
    "`to_torchscript`,\n",
    "to produce our scripted model binary\n",
    "and we don't need to touch our model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udvnx7sBBklY"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "pl.LightningModule.to_torchscript??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXftpJBizrM6"
   },
   "source": [
    "## Alternatives to TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvFh_SW8v4p6"
   },
   "source": [
    "Though it has some sharp edges,\n",
    "TorchScript is a relatively easy to use tool\n",
    "for compiling neural networks written in PyTorch.\n",
    "\n",
    "If you're willing to tolerate more sharp edges,\n",
    "e.g. limited support for certain ops\n",
    "and a higher risk of subtle differences in behavior, the\n",
    "[Open Neural Network eXchange](https://onnx.ai/)\n",
    "format, ONNX, is a compilation target for\n",
    "[a wide variety of DNN libraries](https://onnx.ai/supported-tools.html),\n",
    "from `sklearn` and MATLAB\n",
    "to PyTorch and Hugging Face.\n",
    "\n",
    "A high-level utility for conversion to ONNX is also included\n",
    "in PyTorch Lightning, `pl.LightningModule.to_onnx`.\n",
    "\n",
    "Because it is framework agnostic,\n",
    "there's more and more varied tooling around ONNX,\n",
    "and it has smoother paths to\n",
    "compilation targets that can run DNNs\n",
    "at the highest possible speeds,\n",
    "like\n",
    "[NVIDIA's TensorRT](https://developer.nvidia.com/tensorrt)\n",
    "or\n",
    "[Apache TVM](https://tvm.apache.org/2017/08/17/tvm-release-announcement).\n",
    "\n",
    "TensorRT is the model format used in the\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server),\n",
    "a sort of \"kubernetes for GPU-accelerated DNNs\"\n",
    "that is, as of 2022,\n",
    "the state of the art in running deep networks\n",
    "at maximum throughput on server-grade GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36dKPerevkhZ"
   },
   "source": [
    "## A simple script for compiling and staging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93pc-NLrBR1A"
   },
   "source": [
    "To recap, our model staging workflow,\n",
    "which does the hand-off between training and production, looks like this:\n",
    "\n",
    "1. Get model weights and hyperparameters\n",
    "from a tracked training run in W&B's cloud storage.\n",
    "2. Reload the model as a `LightningModule` using those weights and hyperparameters.\n",
    "3. Call `to_torchscript` on it.\n",
    "4. Save that result to W&B's cloud storage.\n",
    "\n",
    "We provide a simple script to implement this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqgiWO0tFktU"
   },
   "outputs": [],
   "source": [
    "%run training/stage_model.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4qEqMRkFsd4"
   },
   "source": [
    "Here in this notebook,\n",
    "rather than training or scripting a model ourselves,\n",
    "we'll just `--fetch`\n",
    "an already trained and scripted model binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2wfjLmRDwrH"
   },
   "outputs": [],
   "source": [
    "%run training/stage_model.py --fetch --entity=cfrye59 --from_project=fsdl-text-recognizer-2021-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0uNnvjkCZzX"
   },
   "source": [
    "Note that we can use the metadata of the staged model\n",
    "to find the training run that generated the model weights.\n",
    "It requires two graph hops:\n",
    "find the run that created the staged TorchScript model\n",
    "then in that run,\n",
    "find the model checkpoint artifact\n",
    "and look for the run that created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9zJg44hCjRv"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "staged_model_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/artifacts/prod-ready/paragraph-text-recognizer/3e07efa34aec61999c5a/overview\"\n",
    "\n",
    "IFrame(staged_model_url, width=\"100%\", height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're deploying our first model,\n",
    "this doesn't feel that important --\n",
    "it's easy enough to find the training runs\n",
    "we've executed and connect them to the model in production.\n",
    "\n",
    "But as we train and release more models,\n",
    "this information will become harder to find\n",
    "and automation and API access will become more important.\n",
    "\n",
    "This will be especially true if we adopt more sophisticated rollout strategies,\n",
    "like A/B testing or canarying,\n",
    "as the application matures.\n",
    "\n",
    "Our system here is not robust enough to be Enterprise Grade™️ --\n",
    "marking models as \"in production\" is manual\n",
    "and there are no access control planes built in --\n",
    "but at least the information is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our more portable model via a CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7d2WHSCHHHP"
   },
   "source": [
    "Now that our TorchScript model binary file is present,\n",
    "we can spin up our text recognizer\n",
    "with much less code.\n",
    "\n",
    "We just need a compatible version of PyTorch\n",
    "and methods to convert\n",
    "our generic data types\n",
    "(images, strings)\n",
    "to and from PyTorch `Tensor`s.\n",
    "\n",
    "We can put all this together in\n",
    "a single light-weight object,\n",
    "the `ParagraphTextRecognizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGXZep-nDiDk"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.paragraph_text_recognizer import ParagraphTextRecognizer\n",
    "\n",
    "\n",
    "ParagraphTextRecognizer??\n",
    "\n",
    "ptr = ParagraphTextRecognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwVo6BoeGmTW"
   },
   "source": [
    "And from there,\n",
    "we can start running on images\n",
    "and inferring the text that they contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMZlfIoeG3hy"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "example_input = \"text_recognizer/tests/support/paragraphs/a01-077.png\"\n",
    "\n",
    "print(ptr.predict(example_input))\n",
    "Image(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6AHq1TH44Jq"
   },
   "source": [
    "As usual,\n",
    "we write our Python code\n",
    "so that it can be imported as a module\n",
    "and run in a Jupyter notebook,\n",
    "for documentation and experimentation,\n",
    "and we make it executable as a script\n",
    "for easier automation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igY7sd8eGGI3"
   },
   "outputs": [],
   "source": [
    "%run text_recognizer/paragraph_text_recognizer.py --help\n",
    "\n",
    "%run text_recognizer/paragraph_text_recognizer.py {example_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvYmSN0rE2BP"
   },
   "source": [
    "Notice that the `filename` here can be a local file, a URL, or even a cloud storage URI.\n",
    "\n",
    "Rather than writing the logic for handling these different cases,\n",
    "we use the\n",
    "[`smart_open` library](https://pypi.org/project/smart-open/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WQ-P16VC94R"
   },
   "source": [
    "## Testing our model development pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kVq2iBJDZH5"
   },
   "source": [
    "Creating models is _the_ critical function of our code base,\n",
    "so it's important that we test it,\n",
    "at the very least with \"smoke tests\" that let us know\n",
    "if the code is completely broken.\n",
    "\n",
    "Right now we have tests for data loading and model training,\n",
    "but no tests for end-to-end model development,\n",
    "which combines data loading, model training, and model compilation.\n",
    "\n",
    "So we add a simple model development test\n",
    "that trains a model for a very small number of steps\n",
    "and then runs our staging script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPkwFxklDA5V",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat training/tests/test_model_development.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ21iRDqFvxj"
   },
   "source": [
    "As a next step to improve the coverage of this test,\n",
    "we might compare the model's outputs\n",
    "on the same inputs before and after compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyXZhgqEvfe9"
   },
   "source": [
    "### Cleaning up artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l22DqhC4GIJT"
   },
   "source": [
    "The final few lines of the testing script mention\n",
    "\"`selecting for deletion`\" some artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbIW5okFGQv7"
   },
   "source": [
    "As we incorporate more of our code into testing\n",
    "and develop more models,\n",
    "the amount of information we are storing on W&B increases.\n",
    "\n",
    "We're already uploading model checkpoints, several gigabytes per model training run,\n",
    "and now we're also looking at uploading several hundred megabytes\n",
    "of model data per execution of our test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7aBCfpuJVJV"
   },
   "source": [
    "Artifact storage is free up to 100GB,\n",
    "but storing more requires a paid account.\n",
    "\n",
    "That means it literally pays to clean up after ourselves.\n",
    "\n",
    "We use a very simple script to select certain artifacts for deletion.\n",
    " \n",
    "> ⚠️ **Don't use this untested demonstration script in important environments!** ⚠️\n",
    "We include options for `-v`erbose output and a `--dryrun` mode,\n",
    "which are both critical for destructive actions that have access\n",
    "to model weights that might cost $1000s to produce.\n",
    "\n",
    "See the `--help` below for more on cleaning up artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hSqzRplITVB"
   },
   "outputs": [],
   "source": [
    "%run training/cleanup_artifacts.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfB38ywTJDMT"
   },
   "source": [
    "## Tuning inference performance on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zau0MRr1FPw-"
   },
   "source": [
    "Apart from compilation to TorchScript,\n",
    "the biggest difference for running the model in production\n",
    "is that now all of our operations occur on the CPU.\n",
    "\n",
    "This is a surprising feature of DNN deployment\n",
    "that's worth thinking about in detail.\n",
    "\n",
    "Why isn't it a given that deep network inference\n",
    "runs on GPUs, when that's so critical for deep network training?\n",
    "\n",
    "First,\n",
    "not many web applications use GPUs,\n",
    "so there aren't nearly as many good tools and techniques\n",
    "for deplyoing GPU-backed services.\n",
    "\n",
    "But there's another, deeper reason:\n",
    "GPUs are not as easy to run efficiently\n",
    "during inference as they are in training.\n",
    "\n",
    "In training,\n",
    "we use static or synthetic datasets\n",
    "and our training code is in charge\n",
    "of the query patterns.\n",
    "\n",
    "In particular,\n",
    "we can request exactly as many inputs\n",
    "as we want to produce a batch\n",
    "that makes optimal use\n",
    "of our expensive GPUs.\n",
    "\n",
    "In production, requests arrive independently,\n",
    "according to the whims of our users.\n",
    "\n",
    "This makes batching challenging,\n",
    "and by far the simplest service architecture\n",
    "just runs on each request as it arrives.\n",
    "\n",
    "But that tanks GPU utilization.\n",
    "\n",
    "GPUs are highly parallel computers,\n",
    "and batch is the easiest dimension to parallelize on --\n",
    "for example, we load the model weights into memory once,\n",
    "use them, and then release the memory.\n",
    "\n",
    "The cell below\n",
    "compares two traces\n",
    "for a GPU-accelerated\n",
    "Text Recognizer model running\n",
    "on a single input and on a batch.\n",
    "\n",
    "For a simple summary,\n",
    "you can compare the two profiles in TensorBoard\n",
    "([batch size 1 here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-labs-lab05_training/runs/1vj48h6j/tensorboard?workspace=user-cfrye59),\n",
    "[batch size 16 here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/runs/67j1qxws/tensorboard?workspace=user-cfrye59)).\n",
    "\n",
    "GPU utilization,\n",
    "our baseline metric for model performance,\n",
    "is under 50% with batch size 1,\n",
    "as compared to >90% with batch size 16,\n",
    "which fills up GPU RAM.\n",
    "\n",
    "You can also look through the traces for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_NZPLWa-ZVP"
   },
   "outputs": [],
   "source": [
    "trace_comparison_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-labs-lab05_training/reports/Trace-Comparison-Batch-Size-16-vs-1--VmlldzoyNTg2MTU4\"\n",
    "\n",
    "print(trace_comparison_url)\n",
    "IFrame(src=trace_comparison_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6_U1OyU-Vsi"
   },
   "source": [
    "But performance during inference is not as simple \n",
    "as just \"maximize GPU utilization\".\n",
    "\n",
    "In particular, throughput for the GPU with batch size 16\n",
    "is over 2x better,\n",
    "one example per 8 ms vs\n",
    "one example per 40 ms,\n",
    "but latency is much worse.\n",
    "\n",
    "It takes 140ms to complete the batch of size 16.\n",
    "In the intervening time no examples are completed,\n",
    "and all 16 users are waiting on a response.\n",
    "\n",
    "For comparison,\n",
    "running one example at a time\n",
    "would get the first user's result\n",
    "in just 40 ms,\n",
    "but the total processing time for all 16 examples would be\n",
    "640 ms.\n",
    "\n",
    "For user experience, latency is critical,\n",
    "but for making the most efficient use of hardware,\n",
    "throughput is generally more important.\n",
    "\n",
    "During training, we care much less about latency\n",
    "and much more about computing gradients as fast as possible,\n",
    "so we aim for larger batch sizes.\n",
    "\n",
    "Because of the need for efficient use of hardware,\n",
    "running on single inputs isn't always feasible.\n",
    "\n",
    "The usual solution is to run a queue,\n",
    "which collects up enough requests for a batch.\n",
    "\n",
    "One of the easiest ways to do this as of writing in September 2022 is to use\n",
    "[`cog` by Replicate](https://github.com/replicate/cog),\n",
    "which both solves difficult issues with containerizing\n",
    "models with GPU acceleration \n",
    "and includes, as a beta feature, a built-in Redis queue\n",
    "for batching requests and responses.\n",
    "\n",
    "But note that we can't just run a queue that waits for,\n",
    "say, 16 user requests\n",
    "to build up, then runs them all.\n",
    "If 15 requests come in at once,\n",
    "but then no requests come for an hour,\n",
    "all 15 users will be waiting for an hour\n",
    "for their responses --\n",
    "much worse than just waiting a few hundred extra milliseconds!\n",
    "\n",
    "We need to make sure the queue flushes after a certain amount of time,\n",
    "regardless of how many requests it has received,\n",
    "complicating our implementation.\n",
    "\n",
    "Running single inputs on GPUs\n",
    "and running a naive queue\n",
    "are two different ways it's easy to accidentally tank latency\n",
    "while pursuing efficiency,\n",
    "at least for some fraction of cases.\n",
    "\n",
    "So we stick with CPU inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te-CYidTslPo"
   },
   "source": [
    "# Building a simple model UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kGXwQvjJq32"
   },
   "source": [
    "With compilation,\n",
    "we've moved from a model that can only run\n",
    "in a very special environment\n",
    "and with lots of support code\n",
    "into something lightweight\n",
    "that runs with a simple CLI.\n",
    "\n",
    "If we want users to send data to our model\n",
    "and get useful predictions out,\n",
    "we need to create a UI.\n",
    "\n",
    "But a CLI is not a UI --\n",
    "it's at best the foundation out of which a UI is built.\n",
    "\n",
    "This is not just a concern once the model is finished:\n",
    "a UI is an incredible tool for model debugging.\n",
    "\n",
    "It's hard to overstate the difference between\n",
    "a static, CLI or code-writing workflow\n",
    "for sending information to a model\n",
    "and an interactive interface.\n",
    "\n",
    "When your model is easily accessible on a mobile phone,\n",
    "when you can copy-paste text from elsewhere on your machine or the internet,\n",
    "or when you can upload arbitrary files,\n",
    "the whole range of possible inputs becomes clear\n",
    "in a way that's very hard to replicate with fixed data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S163btePLB1K"
   },
   "source": [
    "Unfortunately, creating a GUI from scratch is not easy,\n",
    "especially in Python.\n",
    "\n",
    "The best tool for GUIs is the browser,\n",
    "but the lingua franca of the browser\n",
    "is JavaScript\n",
    "([for now](https://webassembly.org/)).\n",
    "\n",
    "As full stack deep learning engineers,\n",
    "we're already writing Python with C/C++ acceleration,\n",
    "we're gluing scripts together with Bash,\n",
    "and we need to know enough SQL to talk to databases.\n",
    "\n",
    "Do we now need to learn front-end web development too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSeBo0MzL0H9"
   },
   "source": [
    "In the long term, it's a good investment,\n",
    "and we recommend\n",
    "[The Odin Project](https://www.theodinproject.com/),\n",
    "a free online course and community for learning web development.\n",
    "\n",
    "Their\n",
    "[Foundations course](https://www.theodinproject.com/paths/foundations/courses/foundations#html-foundations),\n",
    "starting from HTML foundations and proceeding\n",
    "through basic CSS\n",
    "and JavaScript,\n",
    "is a great way to dip your toes in\n",
    "and learn enough about building websites and UIs\n",
    "in the browser to be dangerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-7pJcsCL_84"
   },
   "source": [
    "In the short term,\n",
    "we write our frontends in Python libraries\n",
    "that effectively write the frontend JavaScript/CSS/HTML\n",
    "for us.\n",
    "\n",
    "For the past few years,\n",
    "[Streamlit](https://streamlit.io/)\n",
    "has been a popular choice for the busy Python data scientist.\n",
    "\n",
    "It remains a solid choice,\n",
    "and tooling for building complex apps with Streamlit is more mature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xey5gzr5tV51"
   },
   "source": [
    "We use the\n",
    "[`gradio` library](https://gradio.app/),\n",
    "which includes a simple API for wrapping\n",
    "a single Python function into a frontend\n",
    "in addition to a less mature, lower-level API\n",
    "for building apps more flexibly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XvUr7irMHQ6"
   },
   "source": [
    "This iteration of the FSDL codebase\n",
    "includes a new module,\n",
    "`app_gradio`,\n",
    "that makes a simple UI for the Text Recognizer\n",
    "using `gradio`.\n",
    "\n",
    "The core component is a script,\n",
    "`app_gradio/app.py`,\n",
    "that can be used to spin up our model and UI\n",
    "from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2Ra8ot292XX"
   },
   "outputs": [],
   "source": [
    "%run app_gradio/app.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9bP3zFo9_YY"
   },
   "source": [
    "But one very nice feature of `gradio`\n",
    "is that it is designed to run as easily\n",
    "from the notebook as from the command line.\n",
    "\n",
    "Let's import the contents of `app.py`\n",
    "and take a look,\n",
    "then launch our UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vryi5r6gDj6D"
   },
   "outputs": [],
   "source": [
    "from app_gradio import app\n",
    "\n",
    "\n",
    "app.make_frontend??\n",
    "frontend = app.make_frontend(ptr.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `gradio`'s high-level API, `gr.Interface`,\n",
    "to build a UI by wrapping our `ptr.predict` function,\n",
    "defining its inputs\n",
    "(an `Image`)\n",
    "and outputs\n",
    "(a `TextBox`),\n",
    "and specifying some formatting\n",
    "and styling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0HxOukBNn13"
   },
   "source": [
    "\n",
    "\n",
    "We can spin up our UI with the `.launch` method,\n",
    "and now we can interact\n",
    "with the model from inside the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoVFtGbuDlTL"
   },
   "outputs": [],
   "source": [
    "frontend.launch(share=True, width=\"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okcoAW7sM13h"
   },
   "source": [
    "For 72 hours, we can also access the model over the public internet\n",
    "using a URL provided by `gradio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5pEhMECNIT6"
   },
   "outputs": [],
   "source": [
    "print(frontend.share_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYfi-lZqNNZd"
   },
   "source": [
    "You can point your browser to that URL\n",
    "to see what the model looks like as a full-fledged web application,\n",
    "instead of a widget inside the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L5uZCJlOGi4"
   },
   "source": [
    "In addition to this UI,\n",
    "`gradio` also creates a simple REST API,\n",
    "so we can make requests\n",
    "from outside the browser,\n",
    "programmatically,\n",
    "and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOngmAWvQnqg"
   },
   "outputs": [],
   "source": [
    "%env API_URL={frontend.share_url + \"/api\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj6XSur7Nlzf"
   },
   "source": [
    "We can see the details of the API by clicking\n",
    "\"view api\" at the bottom of the Gradio interface.\n",
    "\n",
    "In particular,\n",
    "we can see that the API expects image data in\n",
    "[base64 format](https://developer.mozilla.org/en-US/docs/Glossary/Base64),\n",
    "which encodes binary data as ASCII text\n",
    "so that it can be sent over interfaces that expect ASCII text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igeFyT84WqqG"
   },
   "source": [
    "The line below encodes an image with the `base64` utility,\n",
    "packages it into the appropriate JSON format\n",
    "and uses `echo` to pipe it into a `curl` command.\n",
    "\n",
    "`curl` can be used to make requests to web services at URLs\n",
    "-- here `${API_URL}/predict` --\n",
    "of specific types\n",
    "-- here `POST` --\n",
    "that include `-d`ata\n",
    "and `-H`eaders identifying the format of the data.\n",
    "\n",
    "The response is returned as\n",
    "[string-formatted JSON](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nmRbYQCOd3t"
   },
   "outputs": [],
   "source": [
    "response, = ! \\\n",
    "  (echo -n '{ \"data\": [\"data:image/png;base64,'$(base64 -w0 -i text_recognizer/tests/support/paragraphs/a01-077.png)'\"] }') \\\n",
    "  | curl -s -X POST \"${API_URL}/predict\" -H 'Content-Type: application/json' -d @-\n",
    "  \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLy9z593X4_o"
   },
   "source": [
    "JSON, short for \"JavaScript Object Notation\",\n",
    "is effectively the standard for representing dictionaries\n",
    "when sharing information between applications\n",
    "that may be written in different languages.\n",
    "\n",
    "With the standard library's `json.loads`,\n",
    "we can convert the response into a Python dictionary\n",
    "and then access the response `data` within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL4L8o4KRQLx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "print(json.loads(response)[\"data\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhOc0fgrRtuO"
   },
   "source": [
    "Importantly, the `echo | curl` command\n",
    "does not need to be run from the same machine that is running the model --\n",
    "that's another big win for this UI over the CLI script we ran previously.\n",
    "\n",
    "Try running the command from your own machine,\n",
    "if you are running OS X or Linux,\n",
    "and see if you can get a response.\n",
    "\n",
    "Don't forget to define the `API_URL` environment variable on your machine\n",
    "and download the image file,\n",
    "`text_recognizer/tests/support/paragraphs/a01-077.png`,\n",
    "changing the path if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd1UZiM3ZVWz"
   },
   "source": [
    "Once you're done,\n",
    "turn off the Gradio interface by running the `.close` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVyv6KjxJhEb"
   },
   "outputs": [],
   "source": [
    "frontend.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnJpCdI7SHiX"
   },
   "source": [
    "## Testing our UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added a lot of new functionality here,\n",
    "and some of it is critical to our application.\n",
    "\n",
    "The surface area is too large and\n",
    "the components too complex for testing in depth\n",
    "to  be worth the investment --\n",
    "do we really want to set up a\n",
    "[headless browser](https://www.browserstack.com/guide/what-is-headless-browser-testing)\n",
    "or similar mock test to check whether our README is being loaded properly?\n",
    "\n",
    "So once again, we pick the minimal test that checks whether\n",
    "the core functionality is working:\n",
    "we spin up our frontend and ping the API,\n",
    "making sure we get a response with some `data` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat app_gradio/tests/test_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwUhy-swZndq"
   },
   "source": [
    "## Start here, finish anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTKMGCasMznl"
   },
   "source": [
    "You may be concerned:\n",
    "is `gradio` a children's toy?\n",
    "am I painting myself into a corner\n",
    "by using such a high-level framework and doing web development in Python?\n",
    "shouldn't I be using Ruby On Rails/Angular/React/WhateversNext.js?\n",
    "\n",
    "DALL-E Mini, now\n",
    "[crAIyon](https://www.craiyon.com/),\n",
    "began its life as\n",
    "[a Gradio app](https://huggingface.co/spaces/dalle-mini/dalle-mini)\n",
    "built by FSDL alumnus\n",
    "[Boris Dayma](https://twitter.com/borisdayma).\n",
    "\n",
    "Gradio and similar tools\n",
    "are critical for quickly getting to an MVP\n",
    "and getting useful feedback on your model.\n",
    "\n",
    "Expend your engineering effort on data and training,\n",
    "not frontend interface development,\n",
    "until you're sure you've got something people want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BpPtj6tsP-Y"
   },
   "source": [
    "# Wrapping a model into a model service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ButF0a6PSbMi"
   },
   "source": [
    "We've got an interactive interface for our model\n",
    "that we can share with friends, colleagues,\n",
    "potential users, or stakeholders,\n",
    "which is huge.\n",
    "\n",
    "But we have a problem:\n",
    "our model is running in the same place as our frontend.\n",
    "\n",
    "This is simple,\n",
    "but it ties too many things together.\n",
    "\n",
    "First, it ties together execution of the two components.\n",
    "\n",
    "If the model has a heart attack due to misformatted inputs\n",
    "or some mysterious DNN bug,\n",
    "the server goes down.\n",
    "The same applies in reverse --\n",
    "the only API for the model is provided by `gradio`,\n",
    "so a frontend issue means the model is inaccessible.\n",
    "\n",
    "Additionally, it ties together dependencies,\n",
    "since our server and our model are in the same\n",
    "environment.\n",
    "\n",
    "Lastly, it ties together the hardware used to run our\n",
    "server and our model.\n",
    "\n",
    "That's bad because the server and the model scale differently.\n",
    "Running the server at scale has different memory and computational requirements\n",
    "than does running the model at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNoMc7fRcETy"
   },
   "source": [
    "We could just run another server --\n",
    "even writing it in Gradio if we wanted! --\n",
    "for the model.\n",
    "This is common with GPU inference,\n",
    "especially when doing queueing, cacheing,\n",
    "and other advanced techniques for improving\n",
    "model efficiency and latency.\n",
    "\n",
    "But that's potentially expensive --\n",
    "we're running two machines,\n",
    "which costs twice as much.\n",
    "\n",
    "Furthermore, this setup is harder to scale \"horizontally\".\n",
    "\n",
    "We'll pretty quickly need a solution for auto-scaling\n",
    "our two servers independently,\n",
    "e.g. directly in a container orchestration service, like\n",
    "[Kubernetes](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/),\n",
    "or in a managed version of the same, like\n",
    "[Elastic Kubernetes Service](https://aws.amazon.com/eks/),\n",
    "or with an infrastructure automation tool, like\n",
    "[Terraform](https://www.terraform.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WI0H6Imcz_h"
   },
   "source": [
    "Luckily, there is an easier way, because our model service-plus-UI\n",
    "combo fits into a common pattern.\n",
    "\n",
    "We have a server that we want to be up all the time,\n",
    "ready to take requests,\n",
    "but we really only need\n",
    "the model service to run when a request hits.\n",
    "\n",
    "And apart from its environment (which includes the weights),\n",
    "the model only needs the request in order to produce a result.\n",
    "\n",
    "It does not need to hold onto any information in between executions --\n",
    "it is _stateless_.\n",
    "\n",
    "This pattern is common enough that all cloud providers\n",
    "offer a solution that takes the pain out of scaling\n",
    "the stateless component:\n",
    "\"serverless cloud functions\",\n",
    "so named because\n",
    "- they are run intermittently, rather than 24/7, like a server.\n",
    "- they are run on cloud infrastructure.\n",
    "- they are, as in\n",
    "[purely functional programming](https://en.wikipedia.org/wiki/Purely_functional_programming)\n",
    "or in mathematics, \"pure\" functions of their inputs,\n",
    "with no concept of state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE_FhWxLhhxG"
   },
   "source": [
    "We use AWS's serverless offering,\n",
    "[AWS Lambda](https://aws.amazon.com/lambda/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw3Una-yJ_mP"
   },
   "outputs": [],
   "source": [
    "from api_serverless import api\n",
    "\n",
    "api??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGAeXmfFiYOi"
   },
   "source": [
    "Our main function here, `api.handler`, wraps `ParagraphTextRecognizer.predict`.\n",
    "\n",
    "Effectively, `api.handler` maps HTTP requests (`event`s) with AWS's canonical format\n",
    "to a format our `ParagraphTextRecognizer` understands,\n",
    "then converts the text recognizer's output into something\n",
    "that AWS understands.\n",
    "\n",
    "Deploying models as web services is an exercise in taking\n",
    "the Tensor-to-Tensor-mappings we work with in model development\n",
    "and wrapping them so that they run in the\n",
    "JSON-to-JSON-mapping world of web services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDMPQKXqr7pS"
   },
   "source": [
    "## Talking to a model service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V41-UiMct92x"
   },
   "source": [
    "Setting up a serverless function on AWS requires an account\n",
    "and configuration of permissions,\n",
    "so we'll skip that step and,\n",
    "like Julia Child or Martha Stewart, check out\n",
    "[one that was prepared earlier](https://tvtropes.org/pmwiki/pmwiki.php/Main/OneIPreparedEarlier).\n",
    "\n",
    "The cell below sends a request\n",
    "to a serverless cloud function running on the FSDL AWS account.\n",
    "\n",
    "This request is\n",
    "much like the one we sent to the API provided by `gradio`,\n",
    "but we here construct and send it in Python,\n",
    "using the `requests` library,\n",
    "rather than operating from the command line.\n",
    "\n",
    "When playing around with an API,\n",
    "writing requests and parsing responses \"by hand\"\n",
    "in the command line is helpful,\n",
    "but once we're working on real use cases for the API,\n",
    "we'll want to use higher-level libraries\n",
    "with good code quality and nice integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76HwEP2Vzz3F"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from IPython.display import Image\n",
    "import requests  # the preferred library for writing HTTP requests in Python\n",
    "\n",
    "lambda_url = \"https://3akxma777p53w57mmdika3sflu0fvazm.lambda-url.us-west-1.on.aws/\"\n",
    "image_url = \"https://fsdl-public-assets.s3-us-west-2.amazonaws.com/paragraphs/a01-077.png\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"} \n",
    "payload = json.dumps({\"image_url\": image_url})\n",
    "\n",
    "response = requests.post(  # we POST the image to the URL, expecting a prediction as a response\n",
    "    lambda_url, data=payload, headers=headers)\n",
    "pred = response.json()[\"pred\"]  # the response is also json\n",
    "\n",
    "print(pred)\n",
    "\n",
    "Image(url=image_url, width=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZQ2Dt4URN9o"
   },
   "source": [
    "## Local in the front, serverless in the back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMXWTHt4Pxpr"
   },
   "source": [
    "The primary \"win\" here\n",
    "is that we don't need to run\n",
    "the frontend UI server\n",
    "and the backend model service in\n",
    "the same place.\n",
    "\n",
    "For example,\n",
    "we can run a Gradio app locally\n",
    "but send the images to the serverless function\n",
    "for prediction.\n",
    "\n",
    "Our `app_gradio` implementation supports this via the `PredictorBackend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qZ1K0fwOtYK"
   },
   "outputs": [],
   "source": [
    "serverless_backend = app.PredictorBackend(url=lambda_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NVVU2JEPSpy"
   },
   "source": [
    "Previously, our `PredictorBackend`\n",
    "was just a wrapper around the `ParagraphTextRecognizer` class.\n",
    "\n",
    "By passing a URL,\n",
    "we switch to sending data elsewhere via an HTTP request.\n",
    "\n",
    "This is done by the\n",
    "`_predict_from_endpoint` method,\n",
    "which runs effectively the same code we used\n",
    "to talk to the model service in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtSppJq2O_B_"
   },
   "outputs": [],
   "source": [
    "serverless_backend._predict_from_endpoint??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKA68zxUUO9e"
   },
   "source": [
    "The frontend doesn't care where the inference is getting done or how.\n",
    "\n",
    "A `gradio.Interface`\n",
    "just knows there's a Python function that it invokes and then \n",
    "waits for outputs from.\n",
    "\n",
    "Here, that Python function\n",
    "makes a request to the serverless backend,\n",
    "rather than running the model.\n",
    "\n",
    "Go ahead and try it out!\n",
    "\n",
    "You won't notice a difference,\n",
    "except that the machine you're running this notebook on\n",
    "no longer runs the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEkMzohnOcK0"
   },
   "outputs": [],
   "source": [
    "frontend_serverless_backend = app.make_frontend(serverless_backend.run)\n",
    "\n",
    "frontend_serverless_backend.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XytXrIWVuRFu"
   },
   "source": [
    "# Serving a `gradio` app with `ngrok`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i64HrL1wa7F"
   },
   "source": [
    "We've now got a model service and a web server\n",
    "that we can stand up and scale independently,\n",
    "but we're not quite done yet.\n",
    "\n",
    "First, our URL is controlled by Gradio.\n",
    "\n",
    "Very quickly once we leave the territory of a minimal demo,\n",
    "we'll want that URL to be branded.\n",
    "\n",
    "Relatedly,\n",
    "you may have noticed messages indicating that the public URL\n",
    "from Gradio is only good for 72 hours.\n",
    "\n",
    "That means we'd have to reset our frontend\n",
    "and share a new URL every few days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clsPvqtJu0V0"
   },
   "source": [
    "For projects that are mostly intended as public demos,\n",
    "you might follow the advice from those printed warnings\n",
    "and use\n",
    "[Hugging Face Spaces](https://huggingface.co/docs/hub/spaces)\n",
    "for free, permanent hosting.\n",
    "\n",
    "This frees you from the need to keep the frontend server running.\n",
    "\n",
    "However, note that this requires you to use the Hugging Face Hub\n",
    "as a git repository, alongside or replacing GitHub,\n",
    "which is a non-starter for most engineering organizations.\n",
    "\n",
    "Furthermore, the demo is embedded inside Hugging Face,\n",
    "limiting your control over the look and feel of the UX.\n",
    "\n",
    "The better alternative is to continue to run the frontend server\n",
    "on your own infrastructure and to provide a public URL\n",
    "without relying on Gradio's service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWxKXSSG0yNX"
   },
   "source": [
    "Half of the work is already done for us:\n",
    "the `gradio` frontend is already listening on a port and IP address\n",
    "that is only accessible locally\n",
    "(on `127.0.0.1` or `localhost`, as printed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugupgc1bxQlH"
   },
   "outputs": [],
   "source": [
    "frontend_serverless_backend.local_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWcQa-ks1Ktn"
   },
   "source": [
    "So we can, for example, send `curl` requests locally,\n",
    "i.e. on the same machine as the frontend,\n",
    "and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4JRaVjH0kPw"
   },
   "outputs": [],
   "source": [
    "# we send an improperly formatted request, because we just want to check for a response\n",
    "\n",
    "!curl -X POST {frontend_serverless_backend.local_url}api/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK4-tPGf32Hf"
   },
   "source": [
    "Running the same command on another machine will result in an error --\n",
    "`127.0.0.1` and `localhost` always mean \"on this machine\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eiwa6INa0PGe"
   },
   "source": [
    "So fundamentally,\n",
    "the goal is to take the frontend service\n",
    "running on an IP and port that is only accessible locally\n",
    "and make it accessible globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cuuj13Xk0M0Q"
   },
   "source": [
    "There's some tricky bits here --\n",
    "for example, you'll want to communicate using encryption,\n",
    "i.e. over HTTPS instead of HTTP --\n",
    "that make doing this entirely on your own\n",
    "a bit of a headache.\n",
    "\n",
    "To avoid these issues,\n",
    "we can once again use\n",
    "[`ngrok`](https://ngrok.com/),\n",
    "the service we used to provide access to our Label Studio instance\n",
    "in the data annotation lab.\n",
    "\n",
    "The free tier includes public URLs and secure communication with HTTPS.\n",
    "\n",
    "However, the URL changes each time you relaunch your service,\n",
    "e.g. after an outage or a version update.\n",
    "\n",
    "The paid tier allows for branded domains,\n",
    "simpler authentication with\n",
    "[OAuth](https://oauth.net/),\n",
    "and some basic scaling tools like load balancing.\n",
    "\n",
    "This is what we use for the official FSDL text recognizer at\n",
    "[fsdl-text-recognizer.ngrok.io](https://fsdl-text-recognizer.ngrok.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoKA_VUr4Gf2"
   },
   "source": [
    "To get started, let's\n",
    "set up our `ngrok` credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2jkwdaLZAu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "config_file = ngrok.conf.DEFAULT_NGROK_CONFIG_PATH\n",
    "config_file_exists =  os.path.exists(config_file)\n",
    "config_file_contents = !cat {config_file}\n",
    "\n",
    "auth_token_found = config_file_exists \\\n",
    "    and config_file_contents \\\n",
    "    and \"authtoken\" in config_file_contents[0] \\\n",
    "    and \": exit\" not in config_file_contents  # state if interrupted\n",
    "\n",
    "if not auth_token_found:\n",
    "    print(\"Enter your ngrok auth token, which can be copied from https://dashboard.ngrok.com/auth\")\n",
    "    !ngrok authtoken {getpass.getpass()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3SaBJn14YA_"
   },
   "source": [
    "From there,\n",
    "it's as simple as pointing\n",
    "an `ngrok` tunnel\n",
    "at the port associated with your frontend.\n",
    "\n",
    "> For our purposes, ports are\n",
    "\"places you can listen for messages to your web service\".\n",
    "By separating ports,\n",
    "which are identifiers within a machine,\n",
    "from URLs/IPs,\n",
    "which are identifiers across machines,\n",
    "we can run multiple services on a single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wURZiaA5LkeF"
   },
   "outputs": [],
   "source": [
    "TEXT_RECOGNIZER_PORT = frontend_serverless_backend.server_port\n",
    "\n",
    "https_tunnel = ngrok.connect(TEXT_RECOGNIZER_PORT, bind_tls=True)\n",
    "print(https_tunnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head to the printed `ngrok.io` URL from any device --\n",
    "e.g. a mobile phone --\n",
    "to check out your shiny new ML-powered application UI\n",
    "with serverless backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWYBGHLs5iwN"
   },
   "source": [
    "Running a web service out of a Jupyter notebook is not recommended.\n",
    "\n",
    "`gradio` and `ngrok`\n",
    "can be run from the command line.\n",
    "\n",
    "If you're running the lab locally,\n",
    "just define the `TEXT_RECOGNIZER_PORT`\n",
    "and `LAMBDA_URL` environment variables\n",
    "and then run\n",
    "\n",
    "```bash\n",
    "python app_gradio/app.py --model_url $LAMBDA_URL --model_port $TEXT_RECOGNIZER_PORT\n",
    "```\n",
    "\n",
    "in one terminal\n",
    "and, in a separate terminal,\n",
    "run\n",
    "```bash\n",
    "ngrok $TEXT_RECOGNIZER_PORT https\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nycSygTy-PcQ"
   },
   "source": [
    "and navigate to the printed URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQCpzYzHRGfd"
   },
   "source": [
    "## Launching a server on a cloud instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKKnzQjmQPV8"
   },
   "source": [
    "We are almost, but not quite,\n",
    "to the point of a reasonably professional web service.\n",
    "\n",
    "The last missing piece is that our server is running\n",
    "either on Colab,\n",
    "which has short uptimes and is not intended for serving,\n",
    "or on our own personal machine,\n",
    "which is also likely a few\n",
    "[nines](https://en.wikipedia.org/wiki/High_availability#Percentage_calculation) short of an uptime SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKOuYfpTQR-c"
   },
   "source": [
    "We want to instead run this on a dedicated server,\n",
    "and the simplest way to do so is to spin up a machine in a cloud provider.\n",
    "\n",
    "[Elastic Compute Cloud](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)\n",
    "(aka EC2)\n",
    "is the option in AWS,\n",
    "our chosen cloud provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15NI6gI1746O"
   },
   "source": [
    "To get the server going on another machine,\n",
    "we'll need to `git clone` our library,\n",
    "`pip install` our `prod` requirements,\n",
    "then `--fetch` a model with `training/stage_model.py`,\n",
    "and finally run `ngrok` and `app_gradio/app.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faStq6aV-hci"
   },
   "source": [
    "We can make that process slightly easier\n",
    "by incorporating it into a `Dockerfile`\n",
    "and building a container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1i0M7hR-moU"
   },
   "outputs": [],
   "source": [
    "!cat app_gradio/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jskTeGs9AroE"
   },
   "source": [
    "We can then store the container image in a registry, like\n",
    "[Docker Hub](https://hub.docker.com/)\n",
    "or the container image registry built into our cloud provider, like AWS's\n",
    "[Elastic Container Registry](https://aws.amazon.com/ecr/).\n",
    "\n",
    "Then, setup just means pulling the image down onto the machine\n",
    "we want to run our server from and executing a `docker run` command."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
